#IMPORTING LIBRARIES:

from bs4 import BeautifulSoup
from 
import urllib3
import re
import urllib.request
import numpy as np
import time


#MAIN FUNCTIONS:

# 1 - Extract URL
def get_page(urlpage,element ,html_class):
    req = urllib3.PoolManager()
    res = req.request('GET', urlpage)
    soup = BeautifulSoup(res.data, 'html.parser')
    content = soup.find_all(element ,class_= html_class)   
    return content 
    
# 2 - Extract Data
def get_data(titles):
    #Extracting titles and prices: 
    for i in range(len(titles)):
        if re.match(r'^v.lo',str(titles[i])) or re.match(r'^vtt',str(titles[i])):
            if int(price[i])<=100:
                title_velo.append(titles[i])
                price_velo.append(price[i])
                apercu_velo.append(apercus[i])
                link_velo.append(links[i])
    return title_velo


## RUN the function:

urlpage='https://www.paruvendu.fr/annonces/velo/strasbourg/?p='
content=get_page(urlpage,'div','debarras-annonce')
urlpages=[urlpage+str(i) for i in range(1,10)]


apercus=re.findall('<p>\r\n(.*?)\r',str(content))
links=get_page(urlpage,'a','globann')
price=re.findall('(.*?)€',str(content))
#titles=re.findall(r'title="(.*?)\(',str.lower(str(content))) ## Décalage des titres
titles=re.findall('<h3>(.*,?)\r',str.lower(str(content)))

title_velo=[]
price_velo=[]
link_velo=[]
apercu_velo=[]


get_data(titles)


